
### train_bart.py 
* è¨“ç·´seq2seq translation model 
    * ä»»å‹™æœ‰ å°ç¾…/å°ç¾…æ‹¼éŸ³ -> ä¸­æ–‡ or ä¸­æ–‡å°ç¾…/å°ç¾…æ‹¼éŸ³ï¼Œæ ¹æ“šä»»å‹™åœ¨  ```data_collator```çš„functionå…§é¸æ“‡å°æ‡‰çš„input & label
    * æ ¹æ“šä»»å‹™é¸æ“‡å°æ‡‰çš„ model / tokenizer
        * custom tokenizer çš„è©±å¯ä»¥åƒè€ƒ ```tokenizer/train_tokenizer.py``` åŠ å…¥vocab
        * å¦‚æœtokenizer sizeè·ŸåŸå§‹modelä¸ç¬¦åˆè¦åŠ å…¥ 
        ```
        model.resize_token_embeddings(len(tokenizer))
        # defining trainer using ğŸ¤—
        ````

    * è¨“ç·´æ¡†æ¶æ˜¯åƒè€ƒhuggingfaceçš„trainer frameworkä¾†è¨“ç·´
    
    
### eval_translation.ipynb
evaluate translationä»»å‹™ performance (CER/WER)

### translate_aishell.py
è² è²¬å°‡aishellä¸­æ–‡ scirpt åˆ©ç”¨ seq2seq model è½‰æˆå°ç¾…æ•¸å­—èª¿çš„ code
