{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import json\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "# metric = load_metric(\"bleu\")\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tai_lo': 'beh4 hoo7 hng7 loo7 lai5 e5 iu5 kheh4 kam2 siu7 koo2 khenn1 tsai7 te7 khip4 in2 lat8',\n",
       " 'tai_wen': '要讓遠道來的遊客感受古坑在地魅力'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_pre_fix = '/work/u9296553/aics/seq2seq/data_tailo_number'\n",
    "# data_pre_fix = '/work/u9296553/aics/seq2seq/data'\n",
    "data_pre_fix = '/work/u9296553/aics/seq2seq/data_tgb'\n",
    "\n",
    "fp = open(f'{data_pre_fix}/test_data.json')\n",
    "eval_dataset = json.load(fp)\n",
    "print(len(eval_dataset))\n",
    "eval_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang=\"en_XX\"\n",
    "tgt_lang=\"zh_CN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/work/u9296553/aics/seq2seq/tailo_to_taiwen_100/checkpoint-22500'\n",
    "# model_name  = '/work/u9296553/aics/seq2seq/tailo_to_taiwen_test/checkpoint-17000'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"tw_TL\", tgt_lang=\"tw_TW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/work/u9296553/aics/seq2seq/tailo_to_taiwen_origin_tokenizer_with_unk_resize/checkpoint-1448'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=src_lang, tgt_lang=tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/work/u9296553/aics/seq2seq/checkpoints/TGB_tailo_to_zh/checkpoint-12033'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_collator(features:list):\n",
    "\n",
    "#     labels = [f[\"tai_wen\"] for f in features]\n",
    "#     inputs = [f[\"tai_lo\"] for f in features]\n",
    "\n",
    "#     batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, src_lang=\"tw_TL\", tgt_lang=\"tw_TW\", tgt_texts=labels, max_length=32, max_target_length=32)\n",
    "\n",
    "#     for k in batch:\n",
    "#         batch[k] = torch.tensor(batch[k])\n",
    "\n",
    "#     return batch\n",
    "\n",
    "# data_collator(eval_dataset)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 448\n"
     ]
    }
   ],
   "source": [
    "predictions  = []\n",
    "groundtruth = []\n",
    "\n",
    "batch_size = 64\n",
    "max_length = 48\n",
    "for i in range(0, len(eval_dataset), batch_size):\n",
    "    # input_texts = [eval_dataset[k]['tai_lo'] for k in range(i, i + batch_size) if k < len(eval_dataset)]\n",
    "    # labels = [eval_dataset[k]['tai_wen'] for k in range(i, i + batch_size) if k < len(eval_dataset)]\n",
    "\n",
    "    input_texts = [eval_dataset[k]['tai_wen'] for k in range(i, i + batch_size) if k < len(eval_dataset)]\n",
    "    labels = [eval_dataset[k]['tai_lo'] for k in range(i, i + batch_size) if k < len(eval_dataset)]\n",
    "\n",
    "    \n",
    "\n",
    "    groundtruth.extend(labels)\n",
    "    encoded_input = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    if 'token_type_ids' in encoded_input : del encoded_input['token_type_ids']\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    generated_tokens = model.generate(**encoded_input, early_stopping=True, max_length=max_length)\n",
    "\n",
    "    # generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang], early_stopping=True, max_length=48)\n",
    "    # generated_tokens = model.generate(**encoded_input, forced_bos_token_id=21210, early_stopping=True, max_length=48)\n",
    "\n",
    "    generated_tokens = [i[i != tokenizer.cls_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != tokenizer.sep_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != tokenizer.pad_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != tokenizer.unk_token_id ] for i in generated_tokens]\n",
    "\n",
    "    output_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "\n",
    "    output_texts = [''.join(output_text) for output_text in output_texts]\n",
    "    # output_text = ''.join(output_text)\n",
    "    # print(output_text)\n",
    "    predictions.extend(output_texts)\n",
    "\n",
    "    print(f'\\r {i}', end='')\n",
    "\n",
    "print()\n",
    "# results = metric.compute(predictions=predictions, references=references)\n",
    "# print(results[\"bleu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in1 ui7 a1 tiong1 kap4 a1 kok4 kau1 tai3 ai3 lau7 jiat8', 'bin5 tsiong3 khi3 kip4 pai7 hai7', 'beh4 hoo7 hng7 loo7 lai5 e5 iu5 kheh4 kam2 siu7 koo2 khenn1 tsai7 te7 khip4 in2 lat8', 'ki5 sit8 tsit4 ui7 khai1 long2 e5 siau3 lu2', 'siunn7 be7 kau3 tshu3 tsu2 khuann3 tioh8 tshim1 siu7 kam2 tong7', 'tsun7 tioh8 kong1 si7 e5 lik8 su2 kik8 tann2 piann3 tai5 uan5 jin5 bin5 e5 lik8 su2', 'bok8 tsing5 long2 si7 su2 iong7 te7 ha7 tsui2 im1 tshan5', 'sam1 lip8 sin1 bun5 tai5', 'ka1 siang7 hong1 thai1 sai1 lam5 khi3 liu5 e5 ing2 hiang2', 'iau2 u7 lang5 pang3 phau3 a2 khong3 gi7']\n",
      "['in1 ui7 a1 tiong1 kap4 a1 kok4 kau1 tai3 ai3 lau7 jiat8', 'bin5 tsiong3 khi3 kip4 pai7 hai7', 'beh4 ho7 uan2 to7 lai5 e5 iu5 kheh4 kam2 siu7 koo2 khenn1 tsai7 te7 khip4 in2 lat8', 'ki5 sit8 tsit4 ui7 khai1 kiann7 e5 siau3 lu2', 'siunn7 be7 kau3 tshu3 tsu2 khuann3 tioh8 tshim1 siu7 kam2 tong7', 'khuann3 tioh8 kong1 si7 e5 lik8 su2 tua7 kiok8 teh4 phah4 piann3 tai5 uan5 jin5 bin5 e5 lik4 su2', 'bok8 tsing5 long2 si7 su2 iong7 te7 ha7 tsui2 kuan3 khai3', 'sam1 lip8 sin1 bun5 tai5', 'ka1 siang7 hong1 thai1 sai1 lam5 khi3 liu5 e5 ing2 hiang2', 'iau2 u7 lang5 pang3 phau3 a2 khong3 gi7']\n"
     ]
    }
   ],
   "source": [
    "print(groundtruth[:10])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490\n",
      "['因為阿中和阿國交代要熱鬧', '民眾氣急敗壞', '要讓遠道來的遊客感受古坑在地魅力', '其實這位開朗的少女', '想不到屋主看到深受感動', '看到公視的歷史大劇打拚台灣人民的歷史', '目前都是使用地下水灌溉', '三立新聞台', '加上颱風西南氣流的影響', '還有人放鞭炮抗議']\n",
      "['因為阿中和阿國交代愛熱鬧', '民眾氣急敗壞', '要讓遠路來的遊客感受古坑在地吸引力', '其實這位花都的少女', '想不到屋主看到深受感動', '隨著公視的歷史劇打拼台灣人民的血史', '目前都是使用地下水淹田', '三立新聞台', '加上颱風西南氣流的影響', '還有人放鞭炮抗議']\n"
     ]
    }
   ],
   "source": [
    "predictions = [p.replace(' ','') for p in predictions]\n",
    "print(len(predictions))\n",
    "\n",
    "print(groundtruth[:10])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('./TGB_data.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    # print('------台羅數字調-------')\n",
    "    print(eval_dataset[i]['tai_lo'], file=fp)\n",
    "    # print('-----Pred中文---------')\n",
    "    print(predictions[i], file=fp)\n",
    "    print(groundtruth[i], file=fp)\n",
    "    print('-'*10, file=fp)\n",
    "    # input()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_groundtruth = []\n",
    "\n",
    "replace_map = {\n",
    "    'á' : 'a',\n",
    "    'à' : 'a',\n",
    "    'â' : 'a',\n",
    "    'ǎ' : 'a',\n",
    "    'a̋' : 'a',\n",
    "    'ā' : 'a',\n",
    "    'a̍h' : 'ah',\n",
    "    'a̍' : 'a',\n",
    "\n",
    "    'é' : 'e',\n",
    "    'è' : 'e',\n",
    "    'ê' : 'e',\n",
    "    'ě' : 'e',\n",
    "    'ē' : 'e',\n",
    "    'e̋' : 'e',\n",
    "    'e̍h' : 'eh',\n",
    "    'e̍' : 'e',\n",
    "\n",
    "\n",
    "    'í' : 'i',\n",
    "    'ì' : 'i',\n",
    "    'î' : 'i',\n",
    "    'ǐ' : 'i',\n",
    "    'ī' : 'i',\n",
    "    'i̋' : 'i',\n",
    "    'i̍h' : 'ih',\n",
    "    'i̍' : 'i',\n",
    "\n",
    "\n",
    "    'ó' : 'o',\n",
    "    'ò' : 'o',\n",
    "    'ô' : 'o',\n",
    "    'ǒ' : 'o',\n",
    "    'ō' : 'o',\n",
    "    'ő' : 'o',\n",
    "    'o̍h' : 'oh',\n",
    "    'o̍' : 'o',\n",
    "\n",
    "\n",
    "    'ú' : 'u',\n",
    "    'ù' : 'u',\n",
    "    'û' : 'u',\n",
    "    'ǔ' : 'u',\n",
    "    'ū' : 'u',\n",
    "    'ű' : 'u',\n",
    "    'u̍h' : 'uh',\n",
    "    'u̍' : 'u',\n",
    "\n",
    "\n",
    "    'ḿ' : 'm',\n",
    "    'm̀' : 'm',\n",
    "    'm̂' : 'm',\n",
    "    'm̌' : 'm',\n",
    "    'm̄' : 'm',\n",
    "    'm̋' : 'm',\n",
    "    'm̍h' : 'mh',\n",
    "    'm̍' : 'm',\n",
    "\n",
    "\n",
    "    'ń' : 'n',\n",
    "    'ǹ' : 'n',\n",
    "    'n̂' : 'n',\n",
    "    'ň' : 'n',\n",
    "    'n̄' : 'n',\n",
    "    'n̋' : 'n',\n",
    "    'n̍h' : 'nh',\n",
    "    'n̍' : 'n',\n",
    "\n",
    "}\n",
    "\n",
    "for i in range(len(groundtruth)):\n",
    "    \n",
    "    normalize_gt = groundtruth[i]\n",
    "\n",
    "    for k, v in replace_map.items():\n",
    "        normalize_gt = normalize_gt.replace(k, v)\n",
    "\n",
    "    normal_groundtruth.append(normalize_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['因為阿中和阿國交代要熱鬧', '民眾氣急敗壞', '要讓遠道來的遊客感受古坑在地魅力', '其實這位開朗的少女', '想不到屋主看到深受感動', '看到公視的歷史大劇打拚台灣人民的歷史', '目前都是使用地下水灌溉', '三立新聞台', '加上颱風西南氣流的影響', '還有人放鞭炮抗議']\n",
      "['因為阿中和阿國交代要熱鬧', '民眾氣級敗壞', '要讓遠路來的遊客感受古坑在地魅力', '其實這位開都的少女', '想不到屋主看到深受感動', '當著公視的歷史劇打拼台灣人民的歷歷史', '目前都是使用地下水淹田', '三立新聞台', '加上颱風西南氣流的影響', '還有人放鞭炮抗議']\n"
     ]
    }
   ],
   "source": [
    "print(groundtruth[:10])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17186392629340894\n"
     ]
    }
   ],
   "source": [
    "import editdistance as ed\n",
    "import csv\n",
    "def cer_cal(groundtruth, hypothesis):\n",
    "    err = 0\n",
    "    tot = 0\n",
    "    for p, t in zip(hypothesis, groundtruth):\n",
    "        err += float(ed.eval(p.lower(), t.lower()))\n",
    "        tot += len(t)\n",
    "    return err / tot\n",
    "\n",
    "cer = cer_cal(normal_groundtruth, predictions)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsǹg ji̍p piah kha kah mn̂g phāng\n",
      "鑽入壁跤佮門縫\n",
      "門跤佮門縫\n",
      "tō kám kak khah bē hiah kuânn\n",
      "就感覺較袂遐寒\n",
      "就感覺 就感覺 就感覺\n",
      "thiann tio̍h ba̍k kia̍h siann tō tsai iánn ū lâng king kuè\n",
      "聽著木屐聲就知影有人經過\n",
      "聽著木屐聲就知影有人經過\n",
      "phóo phóo á lah\n",
      "普普仔啦\n",
      "普普仔啦\n",
      "tsia̍h àm pá sé kha āu tshīng ba̍k kia̍h\n",
      "食暗飽洗跤後穿木屐\n",
      "後穿木屐\n",
      "mā ē tàng óo tōo ún á khì tiò hî\n",
      "嘛會當挖杜蚓仔去釣魚\n",
      "去釣魚挖杜的聲挖杜\n",
      "lāi bīn ê kang lâng suah tio̍h khì pa̍t king tshuē lîm sî kang lâi póo thiap bô khì ê ka pan\n",
      "內面的工人煞著去別間揣臨時工來補貼無去的加班\n",
      "我真去別間揣臨時工來補貼無去的加班\n",
      "siōng kài kòo lâng uàn ê tō sī tsi̍t lē tsi̍t hiu\n",
      "上蓋顧人怨的就是一例一休\n",
      "上蓋顧人怨的就是一例一休\n",
      "lîm siù hông koh kóng\n",
      "林秀芃閣講\n",
      "林秀芃閣講\n",
      "tsóng kóng tsi̍t kù\n",
      "總講一句\n",
      "總講一句\n",
      "lâi pin peh pah káu tsa̍p tshit hō tshiánn lâi jī tsa̍p hō kuī tâi pān lí\n",
      "來賓八百九十七號請來二十號櫃台辦理\n",
      "來賓八百九十七號請來二十號櫃台辦理\n",
      "tán kàu guá tha̍k kok sió liáu āu\n",
      "等到我讀國小了後\n",
      "讀國小了後\n",
      "koh beh iōng khàm thôo ê hong sik\n",
      "閣欲用崁塗的方式\n",
      "塗塗用崁塗的方式\n",
      "khiā tī tiûnn guā thè guá ka iû\n",
      "徛佇場外替我加油\n",
      "徛佇場外替我加油\n",
      "guá khuànn tsi̍t nî lâi lô thuân ê sîn kî ián tshut\n",
      "我看一年來勞團的神奇演出\n",
      "我看一年來勞團的神奇演出\n",
      "tsit tsūn ài koh puē ìn nî uē\n",
      "這陣愛閣背印尼話\n",
      "背背行為\n",
      "tāi pōo hūn tshù lāi tsa bóo lâng ka tī tsò\n",
      "大部份厝內查某人家己做\n",
      "大部份厝內查某人家己做\n",
      "tsí iàu tsiàu guân hîng tîng khí\n",
      "只要照原形重起\n",
      "只要照原形重起\n",
      "nā sī bīn tíng khǹg tê kóo sè kháu tiánn á hiânn tê tsú tsia̍h\n",
      "若是面頂囥茶鈷細口鼎仔燃茶煮食\n",
      "若是面頂囥茶鈷細口鼎仔燃茶煮食\n",
      "in uī gún a pa ta̍k pái tńg lâi\n",
      "因為阮阿爸逐擺轉來\n",
      "因為阮阿爸逐擺轉來\n",
      "tâi uân tāi ha̍k kiàn tio̍k kah siânn hiong gián kiù sóo kàu siū lâu khó kiông\n",
      "臺灣大學建築佮城鄉研究所教授劉可強\n",
      "臺灣大學鄉研究所教授劉可強\n",
      "lô thuân sī sè kài tē it ê thian ping thian tsâi\n",
      "勞團是世界第一的天兵天才\n",
      "勞團是世界第一的天兵天才\n",
      "tsi̍t tuā tui huâ hâng uân kang ài tuè i khì\n",
      "一大堆華航員工愛綴伊去\n",
      "一大堆華航員工愛綴伊去\n",
      "it lâi in bē tàng tsham ka pā kang\n",
      "一來𪜶袂當參加罷工\n",
      "一來𪜶袂當參加罷工\n",
      "tshiú nā sio tuā thuí nā sio\n",
      "手若燒大腿若燒\n",
      "手若燒\n",
      "lo̍k ní mn̂g ngiâ má tsóo mih kî to ū\n",
      "鹿耳門迎媽祖物旗都有\n",
      "鹿n煮食快樂都\n",
      "siang tshiú tshiú puânn kīng tiàm kuānn hīnn ē hang\n",
      "雙手手盤楗踮捾耳下烘\n",
      "盤踮捾耳下烘\n",
      "tsok tsiá khu bûn sik\n",
      "作者邱文錫\n",
      "作者邱文錫\n",
      "liú á lak khì su iàu thīnn\n",
      "鈕仔落去需要紩\n",
      "鈕仔落去需要紩\n",
      "m̄ koh kan na tâi uân ū pó liû khah tsiâu tsn̂g ê bûn huà tiûnn kíng\n",
      "毋過干焦臺灣有保留較齊全的文化場景\n",
      "毋過干焦臺灣有保留較齊全的文化場景\n",
      "guá ē kì tit huâ hâng pā kang ê sî\n",
      "我會記得華航罷工的時\n",
      "我會記得用這款歌的台灣翠青\n",
      "jī lâi in uî huán tsi̍t lē tsi̍t hiu\n",
      "二來𪜶違反一例一休\n",
      "違反 huán𪜶違反一例\n",
      "tsîng liân ha̍p kok ki tsū khuân ti̍k pia̍t pò kò uân\n",
      "前聯合國居住權特別報告員\n",
      "前聯合國居住權特別報告員\n",
      "mih kî to ū tō sī siánn mih kî to ū\n",
      "物旗都有就是啥物旗都有\n",
      "物旗都有就是啥物旗都有\n",
      "pau kuat iōng thian kiô thè uānn guân pún tsiànn mn̂g ê sió tshu pho\n",
      "包括用天橋替換原本正門的小趨坡\n",
      "包括用天橋替換原本正門的小趨坡\n",
      "i tō ē kā khang khuè àn leh\n",
      "伊就會共工課按咧\n",
      "伊就工課按咧\n",
      "kok ka huat tián uí guân huē tī jī khòng it lio̍k nî tsa̍p it gue̍h\n",
      "國家發展委員會佇二空一六年十一月\n",
      "國家發展委員會佇二空一六年十一月\n",
      "tō khai tsînn iōng bé ê\n",
      "就開錢用買的\n",
      "開錢用買的\n",
      "it lâi kiann ta̍h tio̍h káu sái n̂g kim\n",
      "一來驚踏著狗屎黃金\n",
      "一來驚踏著狗屎黃金\n",
      "ū lâng lâi ah\n",
      "有人來矣\n",
      "有人來矣\n",
      "tshuā gún kui ke hué á tshut kok tshit thô\n",
      "𤆬阮規家伙仔出國𨑨迌\n",
      "實在有夠管是認知權利\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17547/716826460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in range(len(eval_dataset)):\n",
    "    pred = predictions[i]\n",
    "    pred = ''.join(pred)\n",
    "    ref = references[i][0]\n",
    "    ref = ''.join(ref)\n",
    "\n",
    "    tai_lo = eval_dataset[i]['tai_lo']\n",
    "\n",
    "    print(tai_lo)\n",
    "    print(ref)\n",
    "    print(pred)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(data, GPT_FIX=False):\n",
    "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    \n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        if GPT_FIX:\n",
    "            gpt_input = torch.cat((torch.tensor([tokenizer.cls_token_id]).to(device),pred_ids[pred_ids>0]), 0)\n",
    "            gpt_prob = torch.nn.functional.softmax(lm_model(gpt_input).logits, dim=-1)[:voice_prob.size()[0],:]\n",
    "            comb_pred_ids = torch.argmax(gpt_prob*voice_prob, dim=-1)\n",
    "        else: \n",
    "            comb_pred_ids = torch.argmax(voice_prob, dim=-1)\n",
    "        # for wer\n",
    "        decoded_results.append(processor.decode(comb_pred_ids, skip_special_tokens=True, spaces_between_special_tokens=True))\n",
    "        # for cer\n",
    "        # decoded_results.append(processor.decode(comb_pred_ids, skip_special_tokens=True))\n",
    "\n",
    "    return decoded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance as ed\n",
    "import csv\n",
    "\n",
    "\n",
    "def eval_on_csv(csv_path):\n",
    "    file = open(csv_path)\n",
    "    csvreader = csv.reader(file)\n",
    "    for i, row in enumerate(csvreader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        wav_path = row[0]\n",
    "        label = str(row[1])\n",
    "        groundtruth.append(label)\n",
    "        \n",
    "        vdata = load_file_to_data(wav_path)\n",
    "        pred = predict(vdata, GPT_FIX=False)\n",
    "        \n",
    "        \n",
    "        pred = ''.join(pred)\n",
    "        pred = pred.replace('[UNK]', '@')\n",
    "\n",
    "        hypothesis.append(pred)\n",
    "\n",
    "        # wer = wer_cal(groundtruth, hypothesis)\n",
    "        # print(hypothesis)\n",
    "        # print(groundtruth)\n",
    "        # print(wer)\n",
    "        # input()\n",
    "\n",
    "        print(f'\\r {i}', end='')\n",
    "\n",
    "    wer = wer_cal(groundtruth, hypothesis)\n",
    "    cer = cer_cal(groundtruth, hypothesis)\n",
    "    print()\n",
    "    print(wer)\n",
    "    print(cer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5837\n",
      "0.21773487051897536\n",
      "0.09635377266734389\n",
      "5837 5837\n"
     ]
    }
   ],
   "source": [
    "# # eval\n",
    "# # eval_csv_path = '../data/vol1_vol2_ios_test.csv'\n",
    "eval_csv_path = '/work/u9296553/aics/data/vol1_vol2_condenser_test_tai_lo.csv'\n",
    "\n",
    "\n",
    "groundtruth = []\n",
    "hypothesis = []\n",
    "\n",
    "\n",
    "eval_on_csv(eval_csv_path)\n",
    "\n",
    "\n",
    "print(len(groundtruth), len(hypothesis))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5837 5837\n"
     ]
    }
   ],
   "source": [
    "print(len(groundtruth), len(hypothesis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_groundtruth = []\n",
    "normal_hypothesis = []\n",
    "\n",
    "replace_map = {\n",
    "    'á' : 'a',\n",
    "    'à' : 'a',\n",
    "    'â' : 'a',\n",
    "    'ǎ' : 'a',\n",
    "    'a̋' : 'a',\n",
    "    'ā' : 'a',\n",
    "    'a̍h' : 'ah',\n",
    "    'a̍' : 'a',\n",
    "\n",
    "    'é' : 'e',\n",
    "    'è' : 'e',\n",
    "    'ê' : 'e',\n",
    "    'ě' : 'e',\n",
    "    'ē' : 'e',\n",
    "    'e̋' : 'e',\n",
    "    'e̍h' : 'eh',\n",
    "    'e̍' : 'e',\n",
    "\n",
    "\n",
    "    'í' : 'i',\n",
    "    'ì' : 'i',\n",
    "    'î' : 'i',\n",
    "    'ǐ' : 'i',\n",
    "    'ī' : 'i',\n",
    "    'i̋' : 'i',\n",
    "    'i̍h' : 'ih',\n",
    "    'i̍' : 'i',\n",
    "\n",
    "\n",
    "    'ó' : 'o',\n",
    "    'ò' : 'o',\n",
    "    'ô' : 'o',\n",
    "    'ǒ' : 'o',\n",
    "    'ō' : 'o',\n",
    "    'ő' : 'o',\n",
    "    'o̍h' : 'oh',\n",
    "    'o̍' : 'o',\n",
    "\n",
    "\n",
    "    'ú' : 'u',\n",
    "    'ù' : 'u',\n",
    "    'û' : 'u',\n",
    "    'ǔ' : 'u',\n",
    "    'ū' : 'u',\n",
    "    'ű' : 'u',\n",
    "    'u̍h' : 'uh',\n",
    "    'u̍' : 'u',\n",
    "\n",
    "\n",
    "    'ḿ' : 'm',\n",
    "    'm̀' : 'm',\n",
    "    'm̂' : 'm',\n",
    "    'm̌' : 'm',\n",
    "    'm̄' : 'm',\n",
    "    'm̋' : 'm',\n",
    "    'm̍h' : 'mh',\n",
    "    'm̍' : 'm',\n",
    "\n",
    "\n",
    "    'ń' : 'n',\n",
    "    'ǹ' : 'n',\n",
    "    'n̂' : 'n',\n",
    "    'ň' : 'n',\n",
    "    'n̄' : 'n',\n",
    "    'n̋' : 'n',\n",
    "    'n̍h' : 'nh',\n",
    "    'n̍' : 'n',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(groundtruth)):\n",
    "    \n",
    "    normalize_gt = groundtruth[i]\n",
    "    normalize_hp = hypothesis[i]\n",
    "\n",
    "    for k, v in replace_map.items():\n",
    "        normalize_gt = normalize_gt.replace(k, v)\n",
    "        normalize_hp = normalize_hp.replace(k, v)\n",
    "\n",
    "    normal_groundtruth.append(normalize_gt)\n",
    "    normal_hypothesis.append(normalize_hp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5837 5837\n",
      "\n",
      "0.1411325647405123\n",
      "0.06072567378639064\n"
     ]
    }
   ],
   "source": [
    "print(len(normal_groundtruth), len(normal_hypothesis))\n",
    "\n",
    "wer = wer_cal(normal_groundtruth, normal_hypothesis)\n",
    "cer = cer_cal(normal_groundtruth, normal_hypothesis)\n",
    "print()\n",
    "print(wer)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gua e hoo tsio ho be si kiu tshit pat ji pat ji khong kiu khong\n",
      "gua e hoo tsioh ho be si kiu tshit pat ji pat ji khong kiu khong\n",
      "0.0625\n",
      "tsit puann si siann mih tshai\n",
      "tsit puann si siann mih tshai\n",
      "0.0\n",
      "a tsing a li tsit e gin a si an tsuann\n",
      "a tsing a li tsit e gin a si an tsuann\n",
      "0.0\n",
      "siok tsu kin me tshih tiau\n",
      "siok tsu kim me tshi tiau\n",
      "0.3333333333333333\n",
      "than bo tsiah\n",
      "than bo tsiah\n",
      "0.0\n",
      "kam u lang tsai iann bun tsing tshu li e tsing hing\n",
      "kam m u lang tsai iann bun tsing tshu lai e tsing hing\n",
      "0.16666666666666666\n",
      "tioh ai ku ku ah tsiah u tsit nia sin sann thang tshing\n",
      "tioh ai ku a tsiah u tsit iann sin sann thang tsing\n",
      "0.3076923076923077\n",
      "tan bo sann siann\n",
      "tam bo sann sia\n",
      "0.5\n",
      "tiam tiam leh ka lau hu jin lang tau tsing li tsua phe\n",
      "tiam leh ka lau hu lin lang tau tsing li tsua phe\n",
      "0.15384615384615385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_882/2733609945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_hypothesis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwer_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormal_groundtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormal_hypothesis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(normal_groundtruth[i])\n",
    "    print(normal_hypothesis[i])\n",
    "    print(wer_cal([normal_groundtruth[i]], [normal_hypothesis[i]]))\n",
    "    input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aics)",
   "language": "python",
   "name": "aics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
