{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    ")\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead \n",
    "import IPython.display as ipd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = './facebook/wav2vec2-xls-r-300m-vol1_vol2_clean_cleanest_data/checkpoint-1400'\n",
    "# processor_name = \"./facebook/wav2vec2-xls-r-300m-vol1_vol2_clean_cleanest_data/\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ckiplab/gpt2-base-chinese\")  \n",
    "# lm_model = AutoModelWithLMHead.from_pretrained(\"ckiplab/gpt2-base-chinese\").to(device)\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(processor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wav -> 台文\n",
    "# model_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-vol1_vol2_condenser_data/checkpoint-30000'\n",
    "# processor_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-vol1_vol2_condenser_data/'\n",
    "# tokenizer_name = processor_name\n",
    "# device = \"cuda\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(processor_name)\n",
    "# # lm_model = AutoModelWithLMHead.from_pretrained(\"ckiplab/gpt2-base-chinese\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #wav -> 台羅\n",
    "# model_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-1b-vol1_vol2_condenser_tai_lo_no_spec_augment/checkpoint-57860'\n",
    "# processor_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-1b-vol1_vol2_condenser_tai_lo_no_spec_augment/'\n",
    "# tokenizer_name = processor_name\n",
    "# device = \"cuda\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(processor_name)\n",
    "# lm_model = AutoModelWithLMHead.from_pretrained(\"ckiplab/gpt2-base-chinese\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav -> 台羅數字\n",
    "# model_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-fintune_aishell_condenser_tailo_number/checkpoint-2880'\n",
    "# processor_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-fintune_aishell_condenser_tailo_number/'\n",
    "\n",
    "model_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-aishell_tailo_number/checkpoint-4565'\n",
    "processor_name = '/work/u9296553/aics/xls-r-fine-tuning/facebook/wav2vec2-xls-r-300m-aishell_tailo_number/'\n",
    "\n",
    "tokenizer_name = processor_name\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(processor_name)\n",
    "# lm_model = AutoModelWithLMHead.from_pretrained(\"ckiplab/gpt2-base-chinese\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_data(file,sampling_rate=16_000):\n",
    "    batch = {}\n",
    "    speech, _ = torchaudio.load(file)\n",
    "    if sampling_rate != '16_000' or sampling_rate != '16000':\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16_000)\n",
    "        batch[\"speech\"] = resampler.forward(speech.squeeze(0)).numpy()\n",
    "        batch[\"sampling_rate\"] = resampler.new_freq\n",
    "    else:\n",
    "        batch[\"speech\"] = speech.squeeze(0).numpy()\n",
    "        batch[\"sampling_rate\"] = '16000'\n",
    "    return batch\n",
    "\n",
    "def predict_beam(data,beamsize=3):\n",
    "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        sequences = [[[], 1.0]]\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        while True:\n",
    "            all_candidates = list()\n",
    "            exceed = False\n",
    "            for seq in sequences:\n",
    "                tokens, score = seq\n",
    "                gpt_input = torch.tensor([tokenizer.cls_token_id]+tokens).to(device)\n",
    "                gpt_prob = torch.nn.functional.softmax(lm_model(gpt_input).logits, dim=-1)[:len(gpt_input),:]\n",
    "                if len(gpt_input) >= len(voice_prob):\n",
    "                    exceed = True\n",
    "                comb_pred_ids = gpt_prob*voice_prob[:len(gpt_input)]\n",
    "                v,i = torch.topk(comb_pred_ids,50,dim=-1)\n",
    "                for tok_id,tok_prob in zip(i.tolist()[-1],v.tolist()[-1]):\n",
    "                    candidate = [tokens + [tok_id], score + -log(tok_prob)]\n",
    "                    all_candidates.append(candidate)\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "            sequences = ordered[:beamsize]\n",
    "            if exceed:\n",
    "                break\n",
    "\n",
    "        for i in sequences:\n",
    "            decoded_results.append(processor.decode(i[0]))\n",
    "\n",
    "    return decoded_results\n",
    "\n",
    "def predict(data, GPT_FIX=False):\n",
    "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    \n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        if GPT_FIX:\n",
    "            gpt_input = torch.cat((torch.tensor([tokenizer.cls_token_id]).to(device),pred_ids[pred_ids>0]), 0)\n",
    "            gpt_prob = torch.nn.functional.softmax(lm_model(gpt_input).logits, dim=-1)[:voice_prob.size()[0],:]\n",
    "            comb_pred_ids = torch.argmax(gpt_prob*voice_prob, dim=-1)\n",
    "        else: \n",
    "            comb_pred_ids = torch.argmax(voice_prob, dim=-1)\n",
    "        # for wer\n",
    "        pred_str = processor.decode(comb_pred_ids, skip_special_tokens=False, spaces_between_special_tokens=True)\n",
    "        decoded_results.append(pred_str)\n",
    "        # for cer\n",
    "        # decoded_results.append(processor.decode(comb_pred_ids, skip_special_tokens=True))\n",
    "\n",
    "    return decoded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance as ed\n",
    "import csv\n",
    "def cer_cal(groundtruth, hypothesis):\n",
    "    err = 0\n",
    "    tot = 0\n",
    "    for p, t in zip(hypothesis, groundtruth):\n",
    "        err += float(ed.eval(p.lower(), t.lower()))\n",
    "        tot += len(t)\n",
    "    return err / tot\n",
    "\n",
    "def wer_cal(groundtruth, hypothesis):\n",
    "    err = 0\n",
    "    tot = 0\n",
    "    for p, t in zip(hypothesis, groundtruth):\n",
    "        p = p.lower().split(' ')\n",
    "        t = t.lower().split(' ')\n",
    "\n",
    "        err += float(ed.eval(p, t))\n",
    "        tot += len(t)\n",
    "        \n",
    "    return err / tot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5837"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "chars_to_ignore_regex = r\"[¥•＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､　、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·'℃°•·．﹑︰〈〉─《﹖﹣﹂﹁﹔！？｡。＂＃＄％＆＇（）＊＋，﹐－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.．!\\\"#$%&()*+,\\-.\\:;<=>?@\\[\\]\\\\\\/^_`{|}~]\"\n",
    "\n",
    "# eval_csv_path = '/work/u9296553/aics/data/vol1_vol2_condenser_test_tai_lo.csv'\n",
    "# eval_csv_path = '/work/u9296553/aics/data/vol1_condenser_test_tai_lo_number.csv'\n",
    "\n",
    "\n",
    "\n",
    "groundtruth_tailo = []\n",
    "groundtruth_tailo_number = []\n",
    "hypothesis = []\n",
    "tai_wen_label = []\n",
    "\n",
    "\n",
    "file = open(eval_csv_path)\n",
    "csvreader = csv.reader(file)\n",
    "for i, row in enumerate(csvreader):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    wav_path = row[0]\n",
    "    wav_id = wav_path.replace('.wav', '')\n",
    "    json_id = '-'.join(wav_id.split('-')[:-1])\n",
    "    json_path = json_id.replace('-test-master', '-test-key-master').replace('/condenser/wav', '/json')\n",
    "    json_path +=  '.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    tai_wen = json_data['漢羅台文'].replace(',', '')\n",
    "    tai_wen = re.sub(chars_to_ignore_regex, '', tai_wen).lower().replace(\"’\", \"'\")\n",
    "    tai_wen_label.append(tai_wen)\n",
    "\n",
    "    tai_lo = json_data['台羅'].replace(',', '')\n",
    "    tai_lo = tai_lo.replace('--',' ').replace('-', ' ')\n",
    "    tai_lo = re.sub(chars_to_ignore_regex, '', tai_lo).lower().replace(\"’\", \"'\")\n",
    "\n",
    "    tai_lo_number = json_data['台羅數字調'].replace(',', ' ')\n",
    "    tai_lo_number = tai_lo_number.replace('--',' ').replace('-', ' ').replace(\"”\", \" \")\n",
    "    tai_lo_number = re.sub(chars_to_ignore_regex, ' ', tai_lo_number).lower()\n",
    "    tai_lo_number_list = tai_lo_number.split()\n",
    "    tai_lo_number = ' '.join([token for token in tai_lo_number_list if token != ''])\n",
    "    # tai_lo = tai_lo_number\n",
    "\n",
    "    # label = str(row[1])\n",
    "    # groundtruth.append(label)\n",
    "    # groundtruth.append(tai_lo)\n",
    "    groundtruth_tailo.append(tai_lo)\n",
    "    groundtruth_tailo_number.append(tai_lo_number)\n",
    "\n",
    "    \n",
    "    vdata = load_file_to_data(wav_path)\n",
    "    pred = predict(vdata, GPT_FIX=False)\n",
    "    \n",
    "    pred = ''.join(pred)\n",
    "    pred = pred.replace('[UNK]', '@')\n",
    "\n",
    "    hypothesis.append(pred)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # wer = wer_cal(groundtruth, hypothesis)\n",
    "    # print(hypothesis)\n",
    "    # print(groundtruth)\n",
    "    # print(wer)\n",
    "    # input()\n",
    "\n",
    "    print(f'\\r {i}', end='')\n",
    "\n",
    "# wer = wer_cal(groundtruth, hypothesis)\n",
    "# cer = cer_cal(groundtruth, hypothesis)\n",
    "# print()\n",
    "# print(wer)\n",
    "# print(cer)\n",
    "\n",
    "\n",
    "# print(len(groundtruth), len(hypothesis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = groundtruth_tailo_number\n",
    "# groundtruth = groundtruth_tailo\n",
    "# groundtruth = tai_wen_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7176 7176\n",
      "['tsit4 tit8 tsiap4 ing2 hiang2 tioh8 pang5 te7 san2 e5 be2 te7 tsing5 hing5', 'sui1 bong2 i2 king1 huat4 hing5 e5 siann5 tau5 tse3 kng3 e5 hing5 pun2 hu3 sik4 long2 si7 tsing3 siong5 e5', 'koh4 beh4 ui7 kok4 lai7 gua7 un7 tong7 uan5 hian3 siong7 1 tiunn1 pak4 kiann1 siong7 sui2 e5 mia5 phinn3', 'in1 ui7 kuat4 ting7 tshu2 siau1 tiau1 thian1 un1 e5 thoo2 te7 sing5 pau1 hap8 tong5 sua2 tsai1 thoo5 kha1 e5 tshiu7 tsai1', 'tshuan1 tsi1 su1 siang7 pan1 si5 kan1 tua3 tshai2 gu5 lok8 sin5 pi3 jin5 phok8 kng1 si7 tsu1 tsing3 ki3', 'in1 tsha1 put4 to1 long2 si7 kiu2 khong3 au7', 'huat4 hing5 kui1 boo5 iau2 e7 un2 poo7 khok4 tua7', 'pi7 king2 hong1 khak4 jim7 tso3 sin1 long5 tshuan1 tong2 tsi1 poo7 su1 ki3 moo5 ka1 bun5', 'sui1 jian5 kang1 sin3 poo7 tsin1 kin2 to7 san1 tiau7 liau2 au7 puann3 ku3 ue7', 'pak4 kiann1 beh4 ke3 siok8 kai2 tsin3 tshu3 theh8 san2 giap8 hua3 thui1 tsin3 hong1 sik4']\n",
      "['tsit4   tit8   tsiap4   ing2   hiang2   tioh8   pang5   te7   san2 e5 be2   tsing5   hing5', 'sui1   jian5   i2   huat4   hing5 e5 sing5   thau5   tse3   kng3 e5 khuan5   pun2   a2   boo2   long2   si7   tsing3   siong5 e5', 'koh4   ai3   ui7   kok4   lai7   gua7   un7   tong7   uan5   suann3   ting2   1   tiunn1   tiunn1   pak4   kiann1   siang7   sui2 e5   phinn3', 'in1   kuat4   tshua7   tshu2   siau1   tiau3   thian1   in1 e5 thoo2   te7   sing5   pau1   hap8   tong5   it4   tsai1   thoo5   kha1   soo3   biau5', 'tan5   ti1   tshoo1   siong7   pan1   si5   kan1   tshua7   tshai2   gu5   lok8   sin5   pi3   jin5   phok8   kng1   si7   pin5   tsing3   ki3', 'in1   kiong5   beh4   to1   long2   si7   kiu2   khong3   au7', 'huat4   hing5   kui1   boo5   iau2 e7 un2   poo7   khok4   tua7', 'pi7   king2   hong1   suah4   jim7   tso3   sin1   long5   tshuan1   tong2   tsi1   poo7   su1   ki3   moo5   ka1   bun5', 'sui1   jian5   kang1   sin3   poo7   tsin1   kin2   to7   san1   ti5   au7   puann3   ke3   ue7', 'pak4   kiann1   beh4   ke3   siok8   kai2   tsin3   khia7   ke1   san2   giap8   hua3   thui1   tsin3   hong1   sik4']\n"
     ]
    }
   ],
   "source": [
    "print(len(groundtruth), len(hypothesis))\n",
    "print(groundtruth[:10])\n",
    "print(hypothesis[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我的護照號碼是九七八二八二空九空', '這盤是啥物菜', '阿鐘仔你這个囡仔是按怎', '淑珠緊猛揤掉', '趁無食', '敢有人知影文鐘厝裡的情形', '著愛久久矣才有一領新衫通穿', '霆無三聲', '恬恬咧共老婦人人鬥整理紙坯', '想欲留下親情']\n",
      "['我的護照號碼是九七八二八二空九空', '這盤是啥物菜', '阿整仔你tsit个囡仔是按怎', '設主緊猛試牢', '趁無食', '敢有人知影文精厝內的精形', '著hi久仔才有錢領先三通穿', '但無三聲', '恬咧共老婦人鬥整理紙皮', '想欲留下成前']\n",
      "0.33161528736749973\n"
     ]
    }
   ],
   "source": [
    "# hypothesis = [x.replace(' ', '').strip() for x in hypothesis]\n",
    "# print(groundtruth[:10])\n",
    "# print(hypothesis[:10])\n",
    "\n",
    "# cer = cer_cal(groundtruth, hypothesis)\n",
    "# print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tsit4 tit8 tsiap4 ing2 hiang2 tioh8 pang5 te7 san2 e5 be2 te7 tsing5 hing5', 'sui1 bong2 i2 king1 huat4 hing5 e5 siann5 tau5 tse3 kng3 e5 hing5 pun2 hu3 sik4 long2 si7 tsing3 siong5 e5', 'koh4 beh4 ui7 kok4 lai7 gua7 un7 tong7 uan5 hian3 siong7 1 tiunn1 pak4 kiann1 siong7 sui2 e5 mia5 phinn3', 'in1 ui7 kuat4 ting7 tshu2 siau1 tiau1 thian1 un1 e5 thoo2 te7 sing5 pau1 hap8 tong5 sua2 tsai1 thoo5 kha1 e5 tshiu7 tsai1', 'tshuan1 tsi1 su1 siang7 pan1 si5 kan1 tua3 tshai2 gu5 lok8 sin5 pi3 jin5 phok8 kng1 si7 tsu1 tsing3 ki3', 'in1 tsha1 put4 to1 long2 si7 kiu2 khong3 au7', 'huat4 hing5 kui1 boo5 iau2 e7 un2 poo7 khok4 tua7', 'pi7 king2 hong1 khak4 jim7 tso3 sin1 long5 tshuan1 tong2 tsi1 poo7 su1 ki3 moo5 ka1 bun5', 'sui1 jian5 kang1 sin3 poo7 tsin1 kin2 to7 san1 tiau7 liau2 au7 puann3 ku3 ue7', 'pak4 kiann1 beh4 ke3 siok8 kai2 tsin3 tshu3 theh8 san2 giap8 hua3 thui1 tsin3 hong1 sik4']\n",
      "['tsit4 tit8 tsiap4 ing2 hiang2 tioh8 pang5 te7 san2 e5 be2 tsing5 hing5', 'sui1 jian5 i2 huat4 hing5 e5 sing5 thau5 tse3 kng3 e5 khuan5 pun2 a2 boo2 long2 si7 tsing3 siong5 e5', 'koh4 ai3 ui7 kok4 lai7 gua7 un7 tong7 uan5 suann3 ting2 1 tiunn1 tiunn1 pak4 kiann1 siang7 sui2 e5 phinn3', 'in1 kuat4 tshua7 tshu2 siau1 tiau3 thian1 in1 e5 thoo2 te7 sing5 pau1 hap8 tong5 it4 tsai1 thoo5 kha1 soo3 biau5', 'tan5 ti1 tshoo1 siong7 pan1 si5 kan1 tshua7 tshai2 gu5 lok8 sin5 pi3 jin5 phok8 kng1 si7 pin5 tsing3 ki3', 'in1 kiong5 beh4 to1 long2 si7 kiu2 khong3 au7', 'huat4 hing5 kui1 boo5 iau2 e7 un2 poo7 khok4 tua7', 'pi7 king2 hong1 suah4 jim7 tso3 sin1 long5 tshuan1 tong2 tsi1 poo7 su1 ki3 moo5 ka1 bun5', 'sui1 jian5 kang1 sin3 poo7 tsin1 kin2 to7 san1 ti5 au7 puann3 ke3 ue7', 'pak4 kiann1 beh4 ke3 siok8 kai2 tsin3 khia7 ke1 san2 giap8 hua3 thui1 tsin3 hong1 sik4']\n"
     ]
    }
   ],
   "source": [
    "hypothesis = [' '.join(x.split()) for x in hypothesis]\n",
    "print(groundtruth[:10])\n",
    "print(hypothesis[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1910180272663603\n",
      "0.12250532049469438\n"
     ]
    }
   ],
   "source": [
    "wer = wer_cal(groundtruth, hypothesis)\n",
    "cer = cer_cal(groundtruth, hypothesis)\n",
    "print(wer)\n",
    "print(cer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23039\n",
      "1413\n",
      "121\n",
      "{'\\x08luh4', 'thut4', 'onn1', 'phih8', '妳si7', 'ngai7', 'khir3', 'oo9', 'me2', 'tsau7', 'oo33', 'tue2', 'siannh4', 'mi7', 'khiap4', 'uah4', 'bai51', 'nngh4', 'gua9', 'too55', 'tsak4', 'tsuinn7', 'ber2', 'poh4', 'alid', 'hann5', 'tsak8', 'luah8', 'tsher6', 'sam3', '川端康成', 'serh4', 'thit4', 'meh4', 'ker2', 'mi1', '塑化劑', 'kim7', 'nylon', 'khinn1', 'thun5', 'gio7', 'ma3', 'hinn3', '寶特瓶', 'tsam1', 'mngh8', 'liang1', 'pue2', 'lir2', 'her2', 'thiau2', 'liu7', 'lin7', '兩岸一家親', 'hah4', 'khang9', 'tser6', 'liau3', 'thoo1', '習近平', 'luah4', 'hia5', 'ji̍\\x08t', 'thin5', '謝謝', 'sue2', 'ter3', 'pui7', 'nooh4', 'haih4', 'tiak8', 'ker3', 'hann9', 'berh4', '奈須氏', 'lap4', 'khuh4', 'the1', 'hiam3', 'hngh4', 'giau3', 'hannh4', 'jiau3', 'ter6', 'khuainnh4', 'huann2', 'painn2', 'sa2', 'tshiak8', 'sap8', 'terry', 'jiau5', 'khia1', 'ke5', 'tshe2', 'lim7', 'lio2', 'lang1', 'siunn5', 'honnh4', 'penn3', 'gim5', 'tshinn5', 'gim7', 'her3', 'mainnh', 'kinn5', 'lio7', 'luh4', 'tam3', '陶瓷', 'sann2', 'phann7', 'iáh', 'tsin5', 'tir6', 'thih8', 'kher3', 'tenn1', 'sannh4'}\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))\n",
    "\n",
    "gt_tokens = set()\n",
    "for gt in groundtruth:\n",
    "    gt_tokens.update(gt.split())\n",
    "\n",
    "print(len(gt_tokens))\n",
    "\n",
    "OOV_set = set()\n",
    "for gt_token in gt_tokens:\n",
    "    if gt_token not in vocab:\n",
    "        OOV_set.add(gt_token)\n",
    "print(len(OOV_set))\n",
    "print(OOV_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---OOV occur---\n",
      "alid -> lit8\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "ma3 -> ma7\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "siunn5 -> tshiu5\n",
      "---OOV occur---\n",
      "sap8 -> sat4\n",
      "---OOV occur---\n",
      "sap8 -> sau3\n",
      "---OOV occur---\n",
      "tam3 -> tan3\n",
      "---OOV occur---\n",
      "thin5 -> thing5\n",
      "---OOV occur---\n",
      "sam3 -> sam1\n",
      "---OOV occur---\n",
      "sam3 -> sam1\n",
      "---OOV occur---\n",
      "lin7 -> un7\n",
      "---OOV occur---\n",
      "hiam3 -> t\n",
      "---OOV occur---\n",
      "tenn1 -> te7\n",
      "---OOV occur---\n",
      "khiap4 -> khiam3\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "honnh4 -> ho\n",
      "---OOV occur---\n",
      "thun5 -> tun5\n",
      "---OOV occur---\n",
      "jiau5 -> ing5\n",
      "---OOV occur---\n",
      "tiak8 -> tik4\n",
      "---OOV occur---\n",
      "tshe2 -> tshe1\n",
      "---OOV occur---\n",
      "jit -> jit8\n",
      "---OOV occur---\n",
      "oo9 -> mau7\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "tsak8 -> tsap8\n",
      "---OOV occur---\n",
      "phann7 -> ann7\n",
      "---OOV occur---\n",
      "khuainnh4 -> khuainn3\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "gim5 -> gin5\n",
      "---OOV occur---\n",
      "pui7 -> pui5\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "gio7 -> iunn7\n",
      "---OOV occur---\n",
      "giau3 -> liau2\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "gua9 -> gua2\n",
      "---OOV occur---\n",
      "phih8 -> phinn7\n",
      "---OOV occur---\n",
      "tsam1 -> tsan1\n",
      "---OOV occur---\n",
      "ke5 -> ki5\n",
      "---OOV occur---\n",
      "hinn3 -> hi3\n",
      "---OOV occur---\n",
      "tsam1 -> tsam7\n",
      "---OOV occur---\n",
      "me2 -> e5\n",
      "---OOV occur---\n",
      "onn1 -> oh4\n",
      "---OOV occur---\n",
      "onn1 -> oh8\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> tiam7\n",
      "---OOV occur---\n",
      "liang1 -> tiam7\n",
      "---OOV occur---\n",
      "mi7 -> mih8\n",
      "---OOV occur---\n",
      "mi7 -> bi7\n",
      "---OOV occur---\n",
      "luh4 -> lu2\n",
      "---OOV occur---\n",
      "ma3 -> ma7\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> liang5\n",
      "---OOV occur---\n",
      "liang1 -> nia7\n",
      "---OOV occur---\n",
      "siunn5 -> siong5\n",
      "---OOV occur---\n",
      "thin5 -> thing5\n",
      "---OOV occur---\n",
      "sam3 -> sam1\n",
      "---OOV occur---\n",
      "sam3 -> sang3\n",
      "---OOV occur---\n",
      "sann2 -> sann1\n",
      "---OOV occur---\n",
      "tenn1 -> te7\n",
      "---OOV occur---\n",
      "khiap4 -> khiam3\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "jiau5 -> iau2\n",
      "---OOV occur---\n",
      "jiau5 -> iau2\n",
      "---OOV occur---\n",
      "\bluh4 -> lu7\n",
      "---OOV occur---\n",
      "thun5 -> tun3\n",
      "---OOV occur---\n",
      "jiau5 -> liau7\n",
      "---OOV occur---\n",
      "jiau5 -> giau5\n",
      "---OOV occur---\n",
      "jiau5 -> iau5\n",
      "---OOV occur---\n",
      "tshe2 -> tshenn2\n",
      "---OOV occur---\n",
      "lap4 -> lah4\n",
      "---OOV occur---\n",
      "jiau5 -> jiau2\n",
      "---OOV occur---\n",
      "lin7 -> jin7\n",
      "---OOV occur---\n",
      "oo9 -> bo5\n",
      "---OOV occur---\n",
      "lin7 -> lin5\n",
      "---OOV occur---\n",
      "thun5 -> phun5\n",
      "---OOV occur---\n",
      "tiak8 -> tiann7\n",
      "---OOV occur---\n",
      "tshe2 -> tse7\n",
      "---OOV occur---\n",
      "lap4 -> lat8\n",
      "---OOV occur---\n",
      "hann9 -> pha1\n",
      "---OOV occur---\n",
      "lin7 -> jin7\n",
      "---OOV occur---\n",
      "oo9 -> moo7\n",
      "---OOV occur---\n",
      "jiau3 -> jiok4\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke7\n",
      "---OOV occur---\n",
      "ke5 -> ke3\n",
      "---OOV occur---\n",
      "ke5 -> tak8\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "tsak8 -> tsa2\n",
      "---OOV occur---\n",
      "tsam1 -> tsan5\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "gim5 -> kim1\n",
      "---OOV occur---\n",
      "pui7 -> khui1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "terry -> the2\n",
      "---OOV occur---\n",
      "tsuinn7 -> tsin1\n",
      "---OOV occur---\n",
      "jiau3 -> tsiau3\n",
      "---OOV occur---\n",
      "gua9 -> kuann5\n",
      "---OOV occur---\n",
      "huann2 -> huah4\n",
      "---OOV occur---\n",
      "huann2 -> huah4\n",
      "---OOV occur---\n",
      "phih8 -> phinn7\n",
      "---OOV occur---\n",
      "tsam1 -> sam1\n",
      "---OOV occur---\n",
      "hinn3 -> i\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "honnh4 -> hoo7\n",
      "---OOV occur---\n",
      "jiau5 -> liau5\n",
      "---OOV occur---\n",
      "jiau5 -> liau7\n",
      "---OOV occur---\n",
      "jiau5 -> liau5\n",
      "---OOV occur---\n",
      "tiak8 -> tiam3\n",
      "---OOV occur---\n",
      "jiau5 -> liau5\n",
      "---OOV occur---\n",
      "jiau5 -> liau2\n",
      "---OOV occur---\n",
      "陶瓷 -> tsu5\n",
      "---OOV occur---\n",
      "lim7 -> 7\n",
      "---OOV occur---\n",
      "tiak8 -> liap8\n",
      "---OOV occur---\n",
      "tshe2 -> he2\n",
      "---OOV occur---\n",
      "hann9 -> han\n",
      "---OOV occur---\n",
      "jiau5 -> giau5\n",
      "---OOV occur---\n",
      "oo9 -> looh4\n",
      "---OOV occur---\n",
      "jiau3 -> au5\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "painn2 -> pai2\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "sann2 -> sann1\n",
      "---OOV occur---\n",
      "tsak8 -> tsap8\n",
      "---OOV occur---\n",
      "khuainnh4 -> khuann3\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "gim5 -> lim5\n",
      "---OOV occur---\n",
      "pui7 -> pui5\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "lio7 -> liau5\n",
      "---OOV occur---\n",
      "jiau3 -> ji\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "lin7 -> jin7\n",
      "---OOV occur---\n",
      "gua9 -> gua7\n",
      "---OOV occur---\n",
      "tsam1 -> tsam2\n",
      "---OOV occur---\n",
      "hinn3 -> hi3\n",
      "---OOV occur---\n",
      "liau3 -> liau2\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "tsak8 -> tsap8\n",
      "---OOV occur---\n",
      "tsam1 -> tsam7\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "gim5 -> kim1\n",
      "---OOV occur---\n",
      "pui7 -> pui5\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "iáh -> iau2\n",
      "---OOV occur---\n",
      "tsuinn7 -> sui5\n",
      "---OOV occur---\n",
      "lio7 -> tioh8\n",
      "---OOV occur---\n",
      "jiau3 -> tsiau3\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "gua9 -> gua2\n",
      "---OOV occur---\n",
      "huann2 -> huann7\n",
      "---OOV occur---\n",
      "huann2 -> huann7\n",
      "---OOV occur---\n",
      "phih8 -> phinn7\n",
      "---OOV occur---\n",
      "tsam1 -> tsam2\n",
      "---OOV occur---\n",
      "khuainnh4 -> khuann3\n",
      "---OOV occur---\n",
      "ke5 -> ke1\n",
      "---OOV occur---\n",
      "hinn3 -> hi3\n",
      "---OOV occur---\n",
      "thoo1 -> thau1\n",
      "---OOV occur---\n",
      "khuh4 -> khut4\n",
      "---OOV occur---\n",
      "ma3 -> ma7\n",
      "---OOV occur---\n",
      "pui7 -> pue7\n",
      "---OOV occur---\n",
      "hah4 -> ha1\n",
      "---OOV occur---\n",
      "hah4 -> ha1\n",
      "---OOV occur---\n",
      "tshinn5 -> tsinn5\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "me2 -> be2\n",
      "---OOV occur---\n",
      "mi7 -> bin7\n",
      "---OOV occur---\n",
      "huann2 -> huat4\n",
      "---OOV occur---\n",
      "penn3 -> peh4\n",
      "---OOV occur---\n",
      "meh4 -> me7\n",
      "---OOV occur---\n",
      "luah8 -> juah8\n",
      "---OOV occur---\n",
      "lio2 -> jiok8\n",
      "---OOV occur---\n",
      "sann2 -> siann2\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "tshiak8 -> tshiat4\n",
      "---OOV occur---\n",
      "tshiak8 -> tshiat4\n",
      "---OOV occur---\n",
      "haih4 -> hai7\n",
      "---OOV occur---\n",
      "ngai7 -> gai7\n",
      "---OOV occur---\n",
      "ngai7 -> gai7\n",
      "---OOV occur---\n",
      "uah4 -> ua\n",
      "---OOV occur---\n",
      "uah4 -> h\n",
      "---OOV occur---\n",
      "uah4 -> 4\n",
      "---OOV occur---\n",
      "lin7 -> ling7\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "hngh4 -> tshut4\n",
      "---OOV occur---\n",
      "hngh4 -> ng5\n",
      "---OOV occur---\n",
      "thih8 -> thiat4\n",
      "---OOV occur---\n",
      "thih8 -> thih4\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "lin7 -> lim1\n",
      "---OOV occur---\n",
      "uah4 -> gua2\n",
      "---OOV occur---\n",
      "mi7 -> mi5\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "honnh4 -> hong1\n",
      "---OOV occur---\n",
      "pue2 -> peh8\n",
      "---OOV occur---\n",
      "kim7 -> kim1\n",
      "---OOV occur---\n",
      "haih4 -> thai3\n",
      "---OOV occur---\n",
      "ngai7 -> a2\n",
      "---OOV occur---\n",
      "ngai7 -> gai7\n",
      "---OOV occur---\n",
      "uah4 -> ua\n",
      "---OOV occur---\n",
      "uah4 -> h\n",
      "---OOV occur---\n",
      "uah4 -> gua2\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "thih8 -> tsit4\n",
      "---OOV occur---\n",
      "thih8 -> tik4\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "uah4 -> gua2\n",
      "---OOV occur---\n",
      "liu7 -> iu7\n",
      "---OOV occur---\n",
      "mi7 -> mih8\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "honnh4 -> hong1\n",
      "---OOV occur---\n",
      "berh4 -> gua2\n",
      "---OOV occur---\n",
      "berh4 -> beh4\n",
      "---OOV occur---\n",
      "luah8 -> juah8\n",
      "---OOV occur---\n",
      "berh4 -> put4\n",
      "---OOV occur---\n",
      "ma3 -> ma7\n",
      "---OOV occur---\n",
      "pui7 -> pui5\n",
      "---OOV occur---\n",
      "hah4 -> ha1\n",
      "---OOV occur---\n",
      "hah4 -> ha1\n",
      "---OOV occur---\n",
      "thit4 -> hit4\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "haih4 -> ai\n",
      "---OOV occur---\n",
      "ngai7 -> gai7\n",
      "---OOV occur---\n",
      "ngai7 -> giat8\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "thih8 -> ti7\n",
      "---OOV occur---\n",
      "thih8 -> tih4\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "uah4 -> ua\n",
      "---OOV occur---\n",
      "liu7 -> liu5\n",
      "---OOV occur---\n",
      "mi7 -> mih8\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "pue2 -> peh8\n",
      "---OOV occur---\n",
      "kim7 -> kim1\n",
      "---OOV occur---\n",
      "gim5 -> khin1\n",
      "---OOV occur---\n",
      "the1 -> 1\n",
      "---OOV occur---\n",
      "mi7 -> bin7\n",
      "---OOV occur---\n",
      "kinn5 -> khim5\n",
      "---OOV occur---\n",
      "huann2 -> huann1\n",
      "---OOV occur---\n",
      "penn3 -> peh4\n",
      "---OOV occur---\n",
      "meh4 -> me7\n",
      "---OOV occur---\n",
      "luah8 -> juah8\n",
      "---OOV occur---\n",
      "lio2 -> liok8\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "sann2 -> siann2\n",
      "---OOV occur---\n",
      "gim7 -> king1\n",
      "---OOV occur---\n",
      "poh4 -> pho3\n",
      "---OOV occur---\n",
      "hia5 -> hiann1\n",
      "---OOV occur---\n",
      "gim7 -> gi\n",
      "---OOV occur---\n",
      "nngh4 -> nng7\n",
      "---OOV occur---\n",
      "tser6 -> tso3\n",
      "---OOV occur---\n",
      "tir6 -> ti\n",
      "---OOV occur---\n",
      "sue2 -> sue1\n",
      "---OOV occur---\n",
      "nngh4 -> nng7\n",
      "---OOV occur---\n",
      "tser6 -> tse7\n",
      "---OOV occur---\n",
      "her2 -> ho5\n",
      "---OOV occur---\n",
      "tue2 -> tue3\n",
      "---OOV occur---\n",
      "tsher6 -> tso3\n",
      "---OOV occur---\n",
      "oo33 -> hoo7\n",
      "---OOV occur---\n",
      "too55 -> to7\n",
      "---OOV occur---\n",
      "bai51 -> bai2\n",
      "---OOV occur---\n",
      "lir2 -> li\n",
      "---OOV occur---\n",
      "her3 -> ho2\n",
      "---OOV occur---\n",
      "kinn5 -> inn5\n",
      "---OOV occur---\n",
      "tir6 -> ti\n",
      "---OOV occur---\n",
      "hia5 -> hiann1\n",
      "---OOV occur---\n",
      "her3 -> ho7\n",
      "---OOV occur---\n",
      "her3 -> tho5\n",
      "---OOV occur---\n",
      "huann2 -> huann1\n",
      "---OOV occur---\n",
      "tue2 -> tue3\n",
      "---OOV occur---\n",
      "tue2 -> tui3\n",
      "---OOV occur---\n",
      "khinn1 -> kiann5\n",
      "---OOV occur---\n",
      "thut4 -> 4\n",
      "---OOV occur---\n",
      "tir6 -> ti\n",
      "---OOV occur---\n",
      "luah8 -> juah8\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "nngh4 -> nng7\n",
      "---OOV occur---\n",
      "khir3 -> r\n",
      "---OOV occur---\n",
      "hia5 -> hia7\n",
      "---OOV occur---\n",
      "mngh8 -> mng5\n",
      "---OOV occur---\n",
      "kher3 -> k\n",
      "---OOV occur---\n",
      "lir2 -> lit8\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "tser6 -> tso7\n",
      "---OOV occur---\n",
      "sann2 -> siann2\n",
      "---OOV occur---\n",
      "serh4 -> so3\n",
      "---OOV occur---\n",
      "nngh4 -> nng7\n",
      "---OOV occur---\n",
      "gim7 -> gin5\n",
      "---OOV occur---\n",
      "tir6 -> ti\n",
      "---OOV occur---\n",
      "tshiak8 -> kha1\n",
      "---OOV occur---\n",
      "tshiak8 -> tsiap4\n",
      "---OOV occur---\n",
      "tsher6 -> tshue7\n",
      "---OOV occur---\n",
      "tsher6 -> tshng5\n",
      "---OOV occur---\n",
      "sue2 -> leh4\n",
      "---OOV occur---\n",
      "gim5 -> lim1\n",
      "---OOV occur---\n",
      "the1 -> phainn2\n",
      "---OOV occur---\n",
      "me2 -> gin5\n",
      "---OOV occur---\n",
      "gim5 -> kim3\n",
      "---OOV occur---\n",
      "mi7 -> bin7\n",
      "---OOV occur---\n",
      "huann2 -> huann1\n",
      "---OOV occur---\n",
      "penn3 -> piann3\n",
      "---OOV occur---\n",
      "meh4 -> mue7\n",
      "---OOV occur---\n",
      "luah8 -> juah8\n",
      "---OOV occur---\n",
      "lio2 -> liu3\n",
      "---OOV occur---\n",
      "siannh4 -> siann2\n",
      "---OOV occur---\n",
      "gim5 -> gin5\n",
      "---OOV occur---\n",
      "tshiak8 -> tshiah4\n",
      "---OOV occur---\n",
      "tshiak8 -> tshiah4\n",
      "---OOV occur---\n",
      "poh4 -> pok8\n",
      "---OOV occur---\n",
      "haih4 -> ai\n",
      "---OOV occur---\n",
      "ngai7 -> gai7\n",
      "---OOV occur---\n",
      "ngai7 -> kai1\n",
      "---OOV occur---\n",
      "thiau2 -> thiau1\n",
      "---OOV occur---\n",
      "thiau2 -> hiau2\n",
      "---OOV occur---\n",
      "hngh4 -> phinn7\n",
      "---OOV occur---\n",
      "hngh4 -> hng7\n",
      "---OOV occur---\n",
      "thih8 -> ti7\n",
      "---OOV occur---\n",
      "thih8 -> ti7\n",
      "---OOV occur---\n",
      "tsin5 -> tsin1\n",
      "---OOV occur---\n",
      "uah4 -> ua\n",
      "---OOV occur---\n",
      "lin7 -> ling7\n",
      "---OOV occur---\n",
      "mi7 -> mih8\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n",
      "---OOV occur---\n",
      "lang1 -> lang5\n"
     ]
    }
   ],
   "source": [
    "for hp, gt in zip(hypothesis, groundtruth):\n",
    "    hp_tokens = hp.split()\n",
    "    gt_tokens = gt.split()\n",
    "    if len(hp_tokens) != len(gt_tokens):\n",
    "        continue\n",
    "    for i in range(len(hp_tokens)):\n",
    "        hp_token = hp_tokens[i]\n",
    "        gt_token = gt_tokens[i]\n",
    "        if gt_token in OOV_set:\n",
    "            print('---OOV occur---')\n",
    "            print(f'{gt_token} -> {hp_token}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_groundtruth = []\n",
    "normal_hypothesis = []\n",
    "\n",
    "replace_map = {\n",
    "    'á' : 'a',\n",
    "    'à' : 'a',\n",
    "    'â' : 'a',\n",
    "    'ǎ' : 'a',\n",
    "    'a̋' : 'a',\n",
    "    'ā' : 'a',\n",
    "    'a̍h' : 'ah',\n",
    "    'a̍' : 'a',\n",
    "\n",
    "    'é' : 'e',\n",
    "    'è' : 'e',\n",
    "    'ê' : 'e',\n",
    "    'ě' : 'e',\n",
    "    'ē' : 'e',\n",
    "    'e̋' : 'e',\n",
    "    'e̍h' : 'eh',\n",
    "    'e̍' : 'e',\n",
    "\n",
    "\n",
    "    'í' : 'i',\n",
    "    'ì' : 'i',\n",
    "    'î' : 'i',\n",
    "    'ǐ' : 'i',\n",
    "    'ī' : 'i',\n",
    "    'i̋' : 'i',\n",
    "    'i̍h' : 'ih',\n",
    "    'i̍' : 'i',\n",
    "\n",
    "\n",
    "    'ó' : 'o',\n",
    "    'ò' : 'o',\n",
    "    'ô' : 'o',\n",
    "    'ǒ' : 'o',\n",
    "    'ō' : 'o',\n",
    "    'ő' : 'o',\n",
    "    'o̍h' : 'oh',\n",
    "    'o̍' : 'o',\n",
    "\n",
    "\n",
    "    'ú' : 'u',\n",
    "    'ù' : 'u',\n",
    "    'û' : 'u',\n",
    "    'ǔ' : 'u',\n",
    "    'ū' : 'u',\n",
    "    'ű' : 'u',\n",
    "    'u̍h' : 'uh',\n",
    "    'u̍' : 'u',\n",
    "\n",
    "\n",
    "    'ḿ' : 'm',\n",
    "    'm̀' : 'm',\n",
    "    'm̂' : 'm',\n",
    "    'm̌' : 'm',\n",
    "    'm̄' : 'm',\n",
    "    'm̋' : 'm',\n",
    "    'm̍h' : 'mh',\n",
    "    'm̍' : 'm',\n",
    "\n",
    "\n",
    "    'ń' : 'n',\n",
    "    'ǹ' : 'n',\n",
    "    'n̂' : 'n',\n",
    "    'ň' : 'n',\n",
    "    'n̄' : 'n',\n",
    "    'n̋' : 'n',\n",
    "    'n̍h' : 'nh',\n",
    "    'n̍' : 'n',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(groundtruth)):\n",
    "    \n",
    "    normalize_gt = groundtruth[i]\n",
    "    normalize_hp = hypothesis[i]\n",
    "\n",
    "    for k, v in replace_map.items():\n",
    "        normalize_gt = normalize_gt.replace(k, v)\n",
    "        normalize_hp = normalize_hp.replace(k, v)\n",
    "\n",
    "    normal_groundtruth.append(normalize_gt)\n",
    "    normal_hypothesis.append(normalize_hp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752 2752\n",
      "\n",
      "0.10916782197575628\n",
      "0.04215573115510965\n"
     ]
    }
   ],
   "source": [
    "print(len(normal_groundtruth), len(normal_hypothesis))\n",
    "\n",
    "wer = wer_cal(normal_groundtruth, normal_hypothesis)\n",
    "cer = cer_cal(normal_groundtruth, normal_hypothesis)\n",
    "print()\n",
    "print(wer)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(normal_groundtruth[i])\n",
    "#     print(normal_hypothesis[i])\n",
    "#     print(wer_cal([normal_groundtruth[i]], [normal_hypothesis[i]]))\n",
    "#     input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MBartForConditionalGeneration\n",
    "# translate_model_name = '/work/u9296553/aics/seq2seq/tailo_to_taiwen_origin_tokenizer_with_unk_resize/checkpoint-1448'\n",
    "# src_lang=\"en_XX\"\n",
    "# tgt_lang=\"zh_CN\"\n",
    "# trans_tokenizer = AutoTokenizer.from_pretrained(translate_model_name, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "# device = \"cuda\"\n",
    "# translate_model = MBartForConditionalGeneration.from_pretrained(translate_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BartForConditionalGeneration\n",
    "translate_model_name = '/work/u9296553/aics/seq2seq/checkpoints/fine-tune-TGB-data-on-TAT-tailo-number/checkpoint-1448'\n",
    "# translate_model_name = '/work/u9296553/aics/seq2seq/checkpoints/bart_base_chinese_tailo_token_based/checkpoint-2172'\n",
    "trans_tokenizer = BertTokenizer.from_pretrained(translate_model_name, use_fast=True)\n",
    "device = \"cuda\"\n",
    "translate_model = BartForConditionalGeneration.from_pretrained(translate_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gua2 e5 hoo7 tsiau3 ho7 be2 si7 kiu2 tshit4 pat4 ji7 pat4 ji7 khong3 kiu2 khong3'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5824\n",
      "5837\n"
     ]
    }
   ],
   "source": [
    "predictions_tai_wen  = []\n",
    "batch_size = 64\n",
    "for i in range(0, len(hypothesis), batch_size):\n",
    "    input_texts = hypothesis[i: i + batch_size]\n",
    "\n",
    "    encoded_input = trans_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=48)\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    if 'token_type_ids' in encoded_input : del encoded_input['token_type_ids']\n",
    "    generated_tokens = translate_model.generate(**encoded_input, early_stopping=True, max_length=48)\n",
    "\n",
    "    # generated_tokens = translate_model.generate(**encoded_input, forced_bos_token_id=21210, early_stopping=True, max_length=48)\n",
    "    # generated_tokens = translate_model.generate(**encoded_input, forced_bos_token_id=trans_tokenizer.lang_code_to_id[tgt_lang], early_stopping=True, max_length=48)\n",
    "    generated_tokens = [i[i != trans_tokenizer.cls_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != trans_tokenizer.sep_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != trans_tokenizer.pad_token_id ] for i in generated_tokens]\n",
    "    generated_tokens = [i[i != trans_tokenizer.unk_token_id ] for i in generated_tokens]\n",
    "\n",
    "    output_texts = trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "    predictions_tai_wen.extend(output_texts)\n",
    "\n",
    "\n",
    "    print(f'\\r {i}', end='')\n",
    "\n",
    "print()\n",
    "print(len(predictions_tai_wen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5837\n",
      "5837\n",
      "['我 的 護 照 號 碼 是 九 七 八 二 八 二 空 九 空', '這 半 是 啥 物 菜', '阿 指 仔 你 tsit 个 囡 仔 是 按 怎', '束 子 me 試 牢', '趁 無 食', '敢 有 人 知 影 文 鐘 厝 來 的 情 形', '著 愛 久 久 仔 才 有 一 領 先 三 通 清', '淡 無 三 聲', '恬 恬 咧 共 老 婦 人 人 鬥 整 理 紙 批', '想 欲 留 學 親 情']\n",
      "['我的護照號碼是九七八二八二空九空', '這盤是啥物菜', '阿鐘仔你這个囡仔是按怎', '淑珠緊猛揤掉', '趁無食', '敢有人知影文鐘厝裡的情形', '著愛久久矣才有一領新衫通穿', '霆無三聲', '恬恬咧共老婦人人鬥整理紙坯', '想欲留下親情']\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions_tai_wen))\n",
    "print(len(tai_wen_label))\n",
    "\n",
    "print(predictions_tai_wen[:10])\n",
    "print(tai_wen_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tai_wen_label_tokens = []\n",
    "for s in tai_wen_label:\n",
    "    tokens = trans_tokenizer.tokenize(s)\n",
    "    tai_wen_label_tokens.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 的 護 照 號 碼 是 九 七 八 二 八 二 空 九 空', '這 半 是 啥 物 菜', '阿 指 仔 你 tsit 个 囡 仔 是 按 怎', '束 子 me 試 牢', '趁 無 食', '敢 有 人 知 影 文 鐘 厝 來 的 情 形', '著 愛 久 久 仔 才 有 一 領 先 三 通 清', '淡 無 三 聲', '恬 恬 咧 共 老 婦 人 人 鬥 整 理 紙 批', '想 欲 留 學 親 情']\n",
      "['我 的 護 照 號 碼 是 九 七 八 二 八 二 空 九 空', '這 盤 是 啥 物 菜', '阿 鐘 仔 你 這 个 囡 仔 是 按 怎', '淑 珠 緊 猛 揤 掉', '趁 無 食', '敢 有 人 知 影 文 鐘 厝 裡 的 情 形', '著 愛 久 久 矣 才 有 一 領 新 衫 通 穿', '霆 無 三 聲', '恬 恬 咧 共 老 婦 人 人 鬥 整 理 紙 坯', '想 欲 留 下 親 情']\n",
      "0.2268914935429534\n"
     ]
    }
   ],
   "source": [
    "print(predictions_tai_wen[:10])\n",
    "print(tai_wen_label_tokens[:10])\n",
    "wer = wer_cal(tai_wen_label_tokens, predictions_tai_wen)\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我的護照號碼是九七八二八二空九空', '這半是啥物菜', '阿指仔你tsit个囡仔是按怎', '束子me試牢', '趁無食', '敢有人知影文鐘厝來的情形', '著愛久久仔才有一領先三通清', '淡無三聲', '恬恬咧共老婦人人鬥整理紙批', '想欲留學親情']\n",
      "['我的護照號碼是九七八二八二空九空', '這盤是啥物菜', '阿鐘仔你這个囡仔是按怎', '淑珠緊猛揤掉', '趁無食', '敢有人知影文鐘厝裡的情形', '著愛久久矣才有一領新衫通穿', '霆無三聲', '恬恬咧共老婦人人鬥整理紙坯', '想欲留下親情']\n"
     ]
    }
   ],
   "source": [
    "predictions_tai_wen = [p.replace(' ','') for p in predictions_tai_wen]\n",
    "print(predictions_tai_wen[:10])\n",
    "print(tai_wen_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29097271511810247\n"
     ]
    }
   ],
   "source": [
    "cer = cer_cal(tai_wen_label, predictions_tai_wen)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   48,    30,    32,    70,  1298,   175,   217,    39,    67,    62,\n",
      "           185,    82,    34,    71,    82,    34,    71,   174,    67,    62,\n",
      "           174,     2, 21209]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[    0, 21210,   413,  2572,   677,  1021,  5600,     2]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_884/2216413653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgenerated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "predictions_tai_wen_input_label  = []\n",
    "\n",
    "for i in range(len(groundtruth)):\n",
    "    input_text = groundtruth[i]\n",
    "\n",
    "    encoded_input = trans_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    generated_tokens = translate_model.generate(**encoded_input, forced_bos_token_id=21210, early_stopping=True, max_length=48)\n",
    "    output_text = trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    output_text = ''.join(output_text)\n",
    "    predictions_tai_wen_input_label.append(output_text)\n",
    "\n",
    "    print(f'\\r {i}', end='')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9688691547133684\n"
     ]
    }
   ],
   "source": [
    "cer = cer_cal(tai_wen_label, predictions_tai_wen_input_label)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guá ê hōo tsiò hō bé sī kiú tshit pat jī pat jī khòng kiú khòng\n",
      "二試七字偏遠\n",
      "我的護照號碼是九七八二八二空九空\n",
      "tsit puânn sī siánn mi̍h tshài\n",
      "菜in囝\n",
      "這盤是啥物菜\n",
      "a tsing á lí tsit ê gín á sī án tsuánn\n",
      "囡仔七減二囡仔\n",
      "阿鐘仔你這个囡仔是按怎\n",
      "siok tsu kín mé tshi̍h tiāu\n",
      "才有法度上無相借問\n",
      "淑珠緊猛揤掉\n",
      "thàn bô tsia̍h\n",
      "趁\n",
      "趁無食\n",
      "kám ū lâng tsai iánn bûn tsing tshù lí ê tsîng hîng\n",
      "知影絕竅你是毋是做 庄跤人較抾\n",
      "敢有人知影文鐘厝裡的情形\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_884/1913612653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_tai_wen_input_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtai_wen_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in range(len(hypothesis)):\n",
    "    print(groundtruth[i])\n",
    "    print(predictions_tai_wen_input_label[i])\n",
    "    \n",
    "    print(tai_wen_label[i])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tai_wen_input_label  = []\n",
    "\n",
    "for i in range(len(hypothesis)):\n",
    "    tai_lo_label = groundtruth[i]\n",
    "    input_text = tai_lo_label\n",
    "\n",
    "    encoded_input = trans_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    generated_tokens = translate_model.generate(**encoded_input, forced_bos_token_id=21210, early_stopping=True, max_length=48)\n",
    "    output_text = trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    output_text = ''.join(output_text)\n",
    "    predictions_tai_wen_input_label.append(output_text)\n",
    "\n",
    "    print(f'\\r {i}', end='')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5837\n",
      "5837\n",
      "5837\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions_tai_wen))\n",
    "print(len(tai_wen_label))\n",
    "print(len(hypothesis))\n",
    "print(len(groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guá ê hōo tsio̍h hō bé sī kiú tshit pat jī pat jī khòng kiú khòng\n",
      "guá ê hōo tsiò hō bé sī kiú tshit pat jī pat jī khòng kiú khòng\n",
      "二逼試七\n",
      "二試七字偏遠\n",
      "我的護照號碼是九七八二八二空九空\n",
      "tsit puânn sī siánn mih tshài\n",
      "tsit puânn sī siánn mi̍h tshài\n",
      "菜體的菜\n",
      "菜in囝\n",
      "這盤是啥物菜\n",
      "a tsing á lí tsit ê gín á sī án tsuánn\n",
      "a tsing á lí tsit ê gín á sī án tsuánn\n",
      "囡仔七減二囡仔\n",
      "囡仔七減二囡仔\n",
      "阿鐘仔你這个囡仔是按怎\n",
      "sio̍k tsu kim mé tshì tiâu\n",
      "siok tsu kín mé tshi̍h tiāu\n",
      "頂下立場\n",
      "才有法度上無相借問\n",
      "淑珠緊猛揤掉\n",
      "thàn bô tsia̍h\n",
      "thàn bô tsia̍h\n",
      "趁\n",
      "趁\n",
      "趁無食\n",
      "kám m̄ ū lâng tsai iánn bûn tsing tshù lâi ê tsîng hîng\n",
      "kám ū lâng tsai iánn bûn tsing tshù lí ê tsîng hîng\n",
      "幾个延伸khoo查某的\n",
      "知影絕竅你是毋是做 庄跤人較抾\n",
      "敢有人知影文鐘厝裡的情形\n",
      "tio̍h ài kú á tsiah ū tsi̍t iánn sin sann thang tsing\n",
      "tio̍h ài kú kú ah tsiah ū tsi̍t niá sin sann thang tshīng\n",
      "tī船熟似逼趕的心肝\n",
      "贊成三認真教囝講矣\n",
      "著愛久久矣才有一領新衫通穿\n",
      "tām bô sann siá\n",
      "tân bô sann siann\n",
      "無初見面\n",
      "無聲\n",
      "霆無三聲\n",
      "tiàm leh kā lāu hū lîn lâng tàu tsing lí tsuā phe\n",
      "tiām tiām leh kā lāu hū jîn lâng tàu tsíng lí tsuá phe\n",
      "佇咧真正予無leh懸的山頂\n",
      "國民政府leh舉辦就是台語地質 小姐我beh\n",
      "恬恬咧共老婦人人鬥整理紙坯\n",
      "siūnn beh lâu ha̍k tshing tsîng\n",
      "siūnn beh lâu hā tshin tsîng\n",
      "想欲相借問\n",
      "想欲相tshiū\n",
      "想欲留下親情\n",
      "bûn tsiong sann\n",
      "gōobûn tsiong sann\n",
      "文章三\n",
      "五五\n",
      "五文章三\n",
      "suà lo̍h lâi khuànn tâi pak tshī bîn á tsài tsa̍p tshit hō ê thinn khì\n",
      "suà lo̍h lâi khuànn tâi pak tshī bîn á tsài tsa̍p tshit hō ê thinn khì\n",
      "紲落來看台南市明仔載天氣\n",
      "紲落來看台南市明仔載天氣\n",
      "紲落來看台北市明仔載十七號的天氣\n",
      "tshit gue̍h sî á pài hōo guá siōng pàng bē lī ê iah sī hōo khi beh ê pênn poo tsok kah tâi uân tsoo lîng a lí\n",
      "tshit gue̍h sî á pài pài hōo guá siōng pàng bē lī ê iah sī hông khi bia̍t ê pênn poo tso̍k kah tâi uân tsóo lîng alid\n",
      "揣頭路鏡仔試贊成抑是討論\n",
      "彼隻豬哥文明國家了後ê學者放學仔 só\n",
      "七月時仔拜拜予我上放袂離的猶是hông欺滅的平埔族佮台灣祖靈alid\n",
      "kok hāng koh gia̍p mā lóng ê tshuân thuân lâi pài kiû tsâi lī\n",
      "kok hâng kok gia̍p mā lóng ē tshuân lâi pài pài kiû tsâi lī\n",
      "三認真閣等一下的生活阿花來八八五年\n",
      "伊古亭用等一下收人來八八五年\n",
      "各行各業嘛攏會攢來拜拜求財利\n",
      "i kui khì ôo ooh kiò\n",
      "i kui khì onn onn kiò\n",
      "別種喔\n",
      "onn診\n",
      "伊規氣唔唔叫\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_884/285954135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_tai_wen_input_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtai_wen_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/aics/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in range(len(hypothesis)):\n",
    "    print(hypothesis[i])\n",
    "    print(groundtruth[i])\n",
    "    \n",
    "    print(predictions_tai_wen[i])\n",
    "    print(predictions_tai_wen_input_label[i])\n",
    "    print(tai_wen_label[i])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704372513741478\n"
     ]
    }
   ],
   "source": [
    "cer = cer_cal(tai_wen_label, predictions_tai_wen)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9688691547133684\n"
     ]
    }
   ],
   "source": [
    "cer = cer_cal(tai_wen_label, predictions_tai_wen_input_label)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aics)",
   "language": "python",
   "name": "aics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
